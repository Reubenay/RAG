{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c42660e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Documents\\working with LLM\\benv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small, fast embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"✅ Model loaded!\")\n",
    "print(f\"Model produces {model.get_sentence_embedding_dimension()} dimensional embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The dog is playing in the park\",\n",
    "    \"A puppy is running outside\",\n",
    "    \"The cat is sleeping on the couch\",\n",
    "    \"Python is a programming language\",\n",
    "    \"Machine learning models need data\",\n",
    "    \"I love coding in Python\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f1992",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate embedding\n",
    "embedding = model.encode(sentences)\n",
    "\n",
    "print(f\"Original text: {sentences}\")\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding type: {type(embedding)}\")\n",
    "print(f\"\\nFirst 10 values: {embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59493d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    \n",
    "    Returns a score between -1 and 1 (higher = more similar)\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "print(\"✅ Similarity function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test sentences\n",
    "sentences = [\n",
    "     \"The dog is playing in the park\",\n",
    "    \"A puppy is running outside\",\n",
    "    \"The cat is sleeping on the couch\",\n",
    "    \"Python is a programming language\",\n",
    "    \"Machine learning models need data\",\n",
    "    \"I love coding in Python\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for all sentences\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Compare first sentence to all others\n",
    "print(\"Comparing to: 'The dog is playing in the park'\\n\")\n",
    "for i, sentence in enumerate(sentences):\n",
    "    similarity = cosine_similarity(embeddings[0], embeddings[i])\n",
    "    print(f\"Similarity to '{sentence}'\")\n",
    "    print(f\"Score: {similarity:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff12b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test sentences\n",
    "sentences = [\n",
    "     \"The dog is playing in the park\",\n",
    "    \"A puppy is running outside\",\n",
    "    \"The cat is sleeping on the couch\",\n",
    "    \"Python is a programming language\",\n",
    "    \"Machine learning models need data\",\n",
    "    \"I love coding in Python\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for all sentences\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Compare first sentence to all others\n",
    "print(\"Comparing to: 'Python is a programming language'\\n\")\n",
    "for i, sentence in enumerate(sentences):\n",
    "    similarity = cosine_similarity(embeddings[3], embeddings[i])\n",
    "    print(f\"Similarity to '{sentence}'\")\n",
    "    print(f\"Score: {similarity:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e3fac",
   "metadata": {},
   "source": [
    "Excercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54305a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    Returns a score between -1 and 1 (higher = more similar)\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "class SimpleRetriever:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        Initialize retriever with embedding model.\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.chunks = []\n",
    "        self.embeddings = None\n",
    "\n",
    "    def add_document(self, document, chunk_size):\n",
    "        \"\"\"\n",
    "        Chunk a single document and add its chunks and embeddings to the retriever.\n",
    "        \"\"\"\n",
    "        self.chunks = []\n",
    "        for i in range(0, len(document), chunk_size):\n",
    "            chunk = document[i:i+chunk_size]\n",
    "            chunk = \" \".join(chunk.strip().split())\n",
    "            if chunk:\n",
    "                self.chunks.append(chunk)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        self.embeddings = self.model.encode(self.chunks)\n",
    "\n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        Search for relevant chunks.\n",
    "        \"\"\"\n",
    "        query_embedding = self.model.encode(query)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for i, chunk_emb in enumerate(self.embeddings):\n",
    "            sim = cosine_similarity(query_embedding, chunk_emb)\n",
    "            similarities.append((self.chunks[i], sim))\n",
    "        \n",
    "        # Sort and return top k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "\n",
    "document = \"\"\"\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to\n",
    "the natural intelligence displayed by humans and animals. Leading AI textbooks define\n",
    "the field as the study of intelligent agents: any device that perceives its environment\n",
    "and takes actions that maximize its chance of successfully achieving its goals.\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that focuses on the use of data\n",
    "and algorithms to imitate the way that humans learn, gradually improving its accuracy.\n",
    "Machine learning is an important component of the growing field of data science.\n",
    "\n",
    "Deep learning is part of a broader family of machine learning methods based on artificial\n",
    "neural networks with representation learning. Learning can be supervised, semi-supervised\n",
    "or unsupervised. Deep learning architectures such as deep neural networks, deep belief\n",
    "networks, recurrent neural networks and convolutional neural networks have been applied\n",
    "to fields including computer vision, speech recognition, natural language processing,\n",
    "machine translation, and bioinformatics.\n",
    "\n",
    "Natural language processing is a subfield of linguistics, computer science, and artificial\n",
    "intelligence concerned with the interactions between computers and human language, in\n",
    "particular how to program computers to process and analyze large amounts of natural\n",
    "language data. Challenges in natural language processing frequently involve speech\n",
    "recognition, natural language understanding, and natural language generation.\n",
    "\"\"\"\n",
    "\n",
    "query = \"What is machine learning?\"\n",
    "retriever = SimpleRetriever()\n",
    "chunk_sizes = [100, 200, 400]\n",
    "results_summary = {}\n",
    "\n",
    "print(\"Chunk Size Comparison:\\n\")\n",
    "\n",
    "for size in chunk_sizes:\n",
    "    retriever.add_document(document, chunk_size=size)\n",
    "    results = retriever.search(query, top_k=3)\n",
    "    \n",
    "    print(f\"{'='*30}\")\n",
    "    print(f\"Chunk Size: {size} characters\")\n",
    "    print(f\"{'='*30}\")\n",
    "    print(f\"- Number of chunks: {len(retriever.chunks)}\")\n",
    "    \n",
    "    if results:\n",
    "        top_chunk, top_score = results[0]\n",
    "        print(f\"- Top result: \\\"{top_chunk}\\\"\")\n",
    "        print(f\"- Score: {top_score:.3f}\")\n",
    "        \n",
    "        analysis = \"\"\n",
    "        if size == 100:\n",
    "            analysis = \"Chunks are too small, cutting sentences mid-thought. The top result captures the phrase 'Machine learning is' but lacks the full definition, leading to a less focused answer.\"\n",
    "        elif size == 200:\n",
    "            analysis = \"This size captures complete or near-complete sentences containing the core definition of machine learning. The top result provides a clear, focused answer to the query.\"\n",
    "        elif size == 400:\n",
    "            analysis = \"The chunk is larger and contains the definition, but it also includes less relevant information about data science. While complete, it is less focused than the medium chunk.\"\n",
    "        print(f\"- Analysis: {analysis}\\n\")\n",
    "        \n",
    "        results_summary[size] = {\"top_chunk\": top_chunk, \"score\": top_score}\n",
    "    else:\n",
    "        print(\"- No results found.\\n\")\n",
    "\n",
    "# --- Final Comparison ---\n",
    "\n",
    "print(\"Best chunk size for this use case: **200 characters**\")\n",
    "print(\"because it provided the most **focused** answer. The chunk size was sufficient to capture the complete definition of 'machine learning' in a single chunk without including too much extraneous information, as was the case with the larger chunk size. The smaller chunks were too fragmented to provide a complete answer.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
